---
title: "[2/4] Where is my cup 서버 사이드 개발기 - Web Crawler 및 로깅 그리고 REST API"
date: "2019-02-13"
draft: false
path: "/dev/where-is-my-cup-sprint-2"
category: "dev"
tags: ["express", "rest-api", "pupeteer", "log4js"]
project: "where-is-my-cup"
---

## # Rest API 구현

Restful 한 서버를 구현하기 위해 고민을 많이 했다. Data 의 resource 별로 route 를 나누고 또 해당 route 에서 HTTP method 별로 handler 를 나누어서 route method 를 작성했다. 생각보다 사용자의 인풋이 많이 필요한 경우가 있었고 그에따른 고민을 하게 되었다.

<br />

<span style="color: red;">*특정 리소스에 접근하기 위한 정보를 어떤 형태로 받을것인가?*</span>

**Body 를 사용** : Database 에 작성되는 정보, server 에서 결정한 general 한 정보

`POST` 나 `PUT` 요청에서는 body 를 통해 작성할 데이터를 전송하기 때문에 body 에 필요한 모든 데이터를 담아서 서버로 전달하도록 구현했었다. 그러나 `GET` 혹은 `DELETE` 요청 등은 body 에 담아서 데이터를 전달 받을 수 없기 때문에 많은 양의 정보가 요구될 때 어떤 경로로 받는게 best practice 인지 고민하게 됐다. 

<br />

**Path variable 을 사용** : 리소스의 근간이 되는 정보

`GET /api/resource/:id` 와 같은 라우팅 경로를 지정하여 리소스 중에서 특정 id 를 통해 조회할 수 있도록 하였다. 사용자 위치정보도 `GET /api/resource/:lat/:lng` 와 같은 형식으로 받도록 라우트 메소드를 구현하였는데 여기서 id 와 search query 등 여러 정보도 함께 받아야 하는 상황이 발생했다. 라우팅 경로 뒤에 전부 붙여보았다. 

```javascript
app.get('/api/resource/:id/:query/:lat/:lng');
```

위의 API 작성 방식이 좋지 않다고 판단했고 아래와 같은 정보를 알게됐다.

<br />

**Query String 사용** : 리소스의 sorting 및 filtering 의 기준이 되는 정보 

Naver 검색 페이지 등 여러 포털사이트에서 uri 마지막에 `?query=keyword` 와 같이 되어있는 곳이 많다. 리소스를 검색해서 추려내거나 정렬, 혹은 일정 기준으로 필터링 등을 할 때 필요한 정보들을 주로 query string 로 받도록 구현한다.

<br />

**Header 를 사용** : 요청에대한 정보와 민감할 수 있는 정보

Header 에는 사용자 인증에 필요한 token 이나 사용자의 현재위치에 대한 정보 등 민감할 수 있는 정보를 담아서 통신했다. 위 param 을 통해 받도록 했던 `latitude` 와 `longitute` 를 header 에 담아서 전달했다. 

<br />
<br />

## # Puppeteer

사용자들에게 서울의 모든 카페정보를 제공해야 했다. 데이터를 어떻게 수집할까 고민을 했고 그 중 하나가 데이터 크롤링이었다. Puppeteer 는 node.js 의 모듈 중 하나로 **headless chrome** 을 사용하는 강력한 툴이다. Puppeteer 를 이용해서 html tag 에 접근하여 데이터를 가져오도록 크롤러를 만들었다.

<br />

<span style="color: red;">*있을수도 있고, 없을수도 있는 정보의 크롤링*</span>

특정 태그를 가져오는 부분에서 자꾸 오류가 발생했다. 그래서 해당 페이지가 정적페이지가 아닌 동적페이지라는 가설을 세우고 `wait` 함수로 특정 태그가 생성 될 때 까지 기다리도록 하였다. 

그러자 이번에는 시간초과 오류가 발생하기 시작해서 확인해보니 해당 태그에 있는정보가 어떤 페이지에는 존재하고 어떤 페이지에는 그렇지 않은 경우가 있었다. 이 경우에는 `try` `catch` 문으로 예외처리를 했으며 catch 구문에 해당 url 를 로깅하여 추후에 다시 확인할 수 있도록 구현했다.

<br />

<span style="color: red;">*네트워크 환경, 컴퓨터의 메모리 등 로컬 환경에서의 문제점*</span>

개발하는 장소의 와이파이로 인해 네트워크 연결이 지연되어 접속이 중단되거나 몇십시간씩 컴퓨터를 켜두고 프로그램을 돌리며 동시에 개발작업을 하면서 컴퓨터에게 무리한 작업을 계속 시켜 과부화가 되어 예기치 못한 오류가 발생하는 경우가 많았다. 

물론 에러 로깅을 지속적으로 하고 중간부터 다시시작 할 수 있도록 로직을 구현하여 에러가 발생한 부분에 대한 크롤링을 다시 진행할수도 있지만 많은 부분이 끊긴다던지 아예 프로그램이 종료되면 다시 작업하기 어려운 상황이 발생했다.

로컬에서 발생하는 문제점들 때문에 aws ec2 에 네트워크 환경이 괜찮고 메모리도 나쁘지 않은 컴퓨터를 하나 열어서 크롤링을 돌리는 방법을 생각했고 ssh 통신으로 원격으로 접속하여 크롤링 작업을 한 결과 2 일 만에 56000 건 이상의 카페데이터를 128 건의 에러만 발생하며 모을 수 있었다.

<br />
<br />

## # Log4js

Log4js 모듈은 node.js 의 강력한 로깅 툴이다. 처음 사용하기로 마음 먹은 계기는 *Eslint Airbnb 표준에서 `console.log()` 함수를 지양했기 때문이다.* 처음 환경설정 및 사용법만 제대로 익혀두면 어플리케이션 흐름을 아주 잘 파악할 수 있게 해주는 훌륭한 모듈이라고 생각한다.

<br />

<span style="color: red;">*예기치 못한 오류를 쉽게 파악하고 분석*</span>

데이터 크롤링 과정, 클라이언트 혹은 데이터베이스와의 통신과정, JWT 확인과정이나 추천시스템 등등 서버를 백그라운드에서 지속 켜두게 되면 어디서 어떤 에러가 발생할지 모르는데 그러한 에러들이 발생한 경위나 조건들을 저장된 로깅 파일을 통해 파악하고 분석하는데 도움을 주며 버그를 고치거나 클라이언트에게 피드백을 줄 때 많이 유용하게 사용됐다.

<br />

<span style="color: red;">*다양한 형태의 로그를 통해 어플리케이션 흐름을 쉽게 확인*</span>

Log4js 에는 `TRACE`, `DEBUG`, `INFO`, `WARN`, `ERROR` 총 5개의 로깅 타입이 있다. 각 로깅 타입마다 콘솔에서 확인되는 색깔이 다르며 `getLogger` 함수의 매개변수에 따라 로거의 이름도 지정이 되어 언제 어느 로직에서 어떤 타입의 로깅이 출력되는지 확인하여 어플리케이션의 흐름을 한눈에 파악할 수 있었다.